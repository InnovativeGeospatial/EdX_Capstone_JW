---
title: 'Capstone: Examining environmental elements relative to mortality sites in
  Arizona, USA'
output: pdf_document
date: "December 2024"
---

 

## Introduction

Between 2011 and 2020, the US Border Patrol averaged 80,082 annual apprehensions of undocumented immigrants (UDIs) in the Tucson Sector of the USA–Mexico border (Martinez et al. 2021). This 262-mile border zone, stretching from Yuma County, AZ, to the New Mexico border, has since seen sharp increases in UDI encounters, with 173,400 reported in 2021 and 230,200 in 2022 (US Customs and Border Protection 2022a, b). This rise is concerning, as the Tucson Sector is recognized as an increasingly dangerous migration route. Despite fluctuations in border crossings, the ratio of deaths to crossings has grown (Boyce et al. 2019). Of the more than 3,300 UDI remains recovered in this sector between 1990 and 2020, 87% are linked to environmental exposure in the Sonora Desert, marked by extreme heat, rugged terrain, and scarce surface water (Martinez et al. 2021).

Since environmental exposure is the leading cause of UDI mortality in this sector (Figure 1; Martinez et al. 2021) and remains a central concern for Border Patrol, I examine how environmental features predict the location of UDI remains in a simplified and general manner. Using data from the Pima County Office of the Medical Examiner (PCOME) that maps UDI remains, I build on research emphasizing spatial data within the sector (Giordano and Spradley 2017).

Terrain significantly impacts surface and atmospheric heat and water distribution at local and regional levels, creating environmental stresses that challenge human physiology. Terrain indices—quantitative measures of Earth’s surface derived from satellite-based digital elevation models (DEMs)—are widely applied in Earth sciences (Foster et al. 2019). Since terrain can predict elements like solar radiation and ambient air temperature, it provides a valuable lens to assess the link between the sector’s extreme conditions and UDI mortality.


```{r, message=FALSE, warning = FALSE, echo=FALSE}
# Install the required packages and dataset
Packages <- c(
  "ggplot2", "viridis", "corrplot", "party", "FactoMineR", "factoextra", 
  "MuMIn", "AICcmodavg", "lme4", "drat", "xgboost", 
  "caret", "tidyverse", "dplyr", "plyr", "stringr", 
  "gsubfn", "lubridate", "xts", "fpp2", "sf", 
  "mapview", "raster", "RColorBrewer", "terra", "nnet", "tinytex", "latex2exp", "latexpdf")

for (pkg in Packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  } else {
    library(pkg, character.only = TRUE)  }
}

setwd("C:\\Users\\Justin.White\\OneDrive - afacademy.af.edu\\Desktop\\DownloadTime\\HarvardStats_2024\\DataScience_Capstone_2024\\Capstone_ChooseYourOwn")
AZ_Data <- read.csv("./Capstone_Mortality_Dataset.csv", header=TRUE)
#Read character variables as factors
AZ_Data$PointType <- factor(AZ_Data$PointType)
filtered_data <- AZ_Data %>% #Filter out blank and 'Pending' values from Cause_of_Death

  filter(!is.na(Cause_of_Death) & Cause_of_Death != "" & Cause_of_Death != "Pending" & Cause_of_Death != "Skeletal Remains" & Cause_of_Death != "Undetermined" & Cause_of_Death != "Not Reported")

cause_counts <- filtered_data %>% #Create a summary of the counts for each Cause_of_Death

  dplyr::count(Cause_of_Death) %>%
  dplyr::arrange(desc(n))
```


```{r, message=FALSE, warning = FALSE, echo=FALSE, fig.cap = "Figure 1. Depiction of causes of death of undocumented immigrants. *Data source https://www.humaneborders.org/)*"}
#Create a bar plot using plasma color gradient
ggplot(cause_counts, aes(x = reorder(Cause_of_Death, n), y = n, fill = n)) +
  geom_bar(stat = "identity", color = "black", show.legend = FALSE) +
  coord_flip() +  # Flip the bar plot horizontally
  labs(title = "Cause of Death Among \n Undocumented Immigrants in \n Arizona, USA",
    x = "Cause of Death",
    y = "Frequency" ) +
  theme_minimal(base_size = 12) +   
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    axis.text = element_text(size = 9)) +
  scale_fill_viridis(option = "plasma")  
```



## Analysis

I defined the area using UDI mortality data from the Pima County Office of the Medical Examiner (n = 1,877; Humane Borders, Inc. 2022). This dataset includes mortality sites in Pima, Santa Cruz, Cochise, Graham, and La Paz Counties from 1981 to 2023. I also generated 998 random points for later prediction purposes.

```{r, message=FALSE, warning = FALSE, echo=FALSE, fig.cap = "Figure 2. Map of our dataset: random location vs mortality body location"}
#Map the data to visualize spatial arrangement of mortality sites.
loc_AZ_Data <- st_as_sf(AZ_Data, coords=c("Longitude","Latitude"), crs=4326)
mapview(loc_AZ_Data,
        zcol="PointType",
        cex=4,
        burst=TRUE,
        alpha = 0,
        alpha.regions=0.2)
```
        

My environmental variables were generated via a terrain model (a 10mx10m-pixel digital elevation model) downloaded from the National Map Viewer (https://apps.nationalmap.gov/downloader/), which encompasses the sector's topography and relief, and generates subsequent derivatives, which include for each pixel: slope, elevation, aspect, ruggedness, proportion of the daily amount of sun received at 9:00AM, 2:00PM, and 6:00PM, the duration of direct solar received in hours, and the amount of diffuse radiation, direct radiation, and total solar radiation received during the longest day of the year in WH/m^2. Also included were an estimation of vegetation density using MSAVI2 (Modified Soil-Adjusted Vegetation Index II) and total aridity via potential evapotranspiration from https://www.climatologylab.org/gridmet.html)


Investigating patterns in the data via a basic correlation plot to inform upcoming generalized linear regression models (GLMs) and avoid multicollinearity.

```{r, message=FALSE, warning = FALSE, echo=FALSE, fig.cap = "Figure 3. Correlation plot of continuous variables in the dataset"}
AZ_Data_Cor <- AZ_Data[6:17]

CorTest <- cor(AZ_Data_Cor, use = "complete.obs")

corrplot(CorTest, method = 'circle', order = 'FPC', type = 'lower', diag = FALSE)
 
```


Next, I used AIC models to examine the relative quality of different generalized linear models (GLMs) by comparing their goodness-of-fit while penalizing for model complexity, thereby helping to identify the most parsimonious model that can be used to predict the difference between a mortality location and a random location. 

As depicted in the AICc table below, the model (labelled AZ_Mod.9) constructed as: PointType ~ DirectSolarDuration + I(DirectSolarDuration^2) + TerrainRoughness yielded the best fit.


```{r, message=TRUE, warning = FALSE, echo=FALSE}

AZ_Data_AIC <- AZ_Data[5:18]
set.seed(123)
#Split the data for GLM_training purposes. This yields an 85/15% split of the data
GLM_train <- createDataPartition(AZ_Data_AIC$PointType, p = 0.85, list = FALSE, times = 1)  
GLM_training <- AZ_Data_AIC[GLM_train,]
GLM_testing <-AZ_Data_AIC[ -GLM_train,]  

AZ_Mod.1 <-glm(PointType ~ Rough + I(Rough ^2), data = GLM_training, family = binomial) #include quadratic pattern of variable
AZ_Mod.2 <-glm(PointType ~ DiffuseRad + I(DiffuseRad ^2) + MSAVI2, data = GLM_training, family = binomial)
AZ_Mod.3 <-glm(PointType ~ Rough + I(Rough ^2) + TotalSolar + I(TotalSolar ^2), data = GLM_training, family = binomial)
AZ_Mod.4 <-glm(PointType ~ RelCont9am + I(RelCont9am ^2), data = GLM_training, family = binomial)
AZ_Mod.5 <-glm(PointType ~ RelCont9am + I(RelCont9am ^2) + Rough, data = GLM_training, family = binomial)
AZ_Mod.6 <-glm(PointType ~ RelCont6pm + I(RelCont6pm ^2) + Slope, data = GLM_training, family = binomial)
AZ_Mod.7 <-glm(PointType ~ TotalSolar + I(TotalSolar ^2), data = GLM_training, family = binomial)
AZ_Mod.8 <-glm(PointType ~ DirecDurat + I(DirecDurat ^2), data = GLM_training, family = binomial)
AZ_Mod.9 <-glm(PointType ~ DirecDurat + I(DirecDurat ^2) + Rough , data = GLM_training, family = binomial)
AZ_Mod.11 <-glm(PointType ~ TotalSolar, data = GLM_training, family = binomial)
AZ_Mod.10 <-glm(PointType ~ TotalSolar + TotAveArid, data = GLM_training, family = binomial)
AZ_Mod.12 <-glm(PointType ~ MSAVI2 + I(MSAVI2 ^2),  data = GLM_training, family = binomial)
AZ_Mod.13 <-glm(PointType ~ TotAveArid, data = GLM_training, family = binomial)
AZ_Mod.14 <-glm(PointType ~ -1, data = GLM_training, family = binomial)

AZ_Cands <- list(AZ_Mod.1, AZ_Mod.2, AZ_Mod.3, AZ_Mod.4, AZ_Mod.5, AZ_Mod.6, AZ_Mod.7, AZ_Mod.8, AZ_Mod.9, AZ_Mod.10, AZ_Mod.11, AZ_Mod.12, AZ_Mod.13, AZ_Mod.14)

AZ_Mods <- c("AZ_Mod.1", "AZ_Mod.2", "AZ_Mod.3", "AZ_Mod.4", "AZ_Mod.5", "AZ_Mod.6",  "AZ_Mod.7", "AZ_Mod.8", "AZ_Mod.9", "AZ_Mod.10", "AZ_Mod.11", "AZ_Mod.12", "AZ_Mod.13", "AZ_Mod.14")

AZ_aictable <- aictab(AZ_Cands, AZ_Mods, sort=T)

AZ_aictable

#VIFs
car::vif(AZ_Mod.9)

#Results:
summary(AZ_Mod.9)

```


I then tested the predictive accuracy of the optimal GLM. VIFs are low (except for quadratic version of variable).


```{r, message=TRUE, warning = FALSE, echo=FALSE}
# Ensure test labels are read as factors
GLM_test_labels <- as.factor(GLM_testing$PointType) 

# Predicted probabilities
GLM_pred_probs <- predict(AZ_Mod.9, newdata = GLM_testing, type = "response")

GLM_pred_labels <- ifelse(GLM_pred_probs > 0.3, "BodyLocation", "RandomLocation")

# Convert predictions to factor with consistent levels
GLM_pred_labels <- factor(GLM_pred_labels, levels = c("BodyLocation", "RandomLocation"))

GLM_conf_matrix <- confusionMatrix(GLM_pred_labels, GLM_test_labels)
print(GLM_conf_matrix)

```

Continuing analyses, I also utilized the Extreme Gradient Boosting Machine Learning Application, XGBOOST, which is useful as it efficiently supports regularization which helps avoid overfitting, to attempt to predict mortality sites vs random locations using environmental metrics.


```{r, message=TRUE, warning = FALSE, echo=FALSE, fig.cap = c("Figure 4. Training log-loss metric indicating training (blue) & testing (red) accuracy", "Figure 5. Feature importance as ranked by the gain metric.")}
AZ_Data_Boost <- AZ_Data_AIC
set.seed(123)

#Split the data for Boost_training purposes. This yields a 70/30% split of the data
Boost_train <- createDataPartition(AZ_Data_Boost$PointType, p = 0.8, list = FALSE, times = 1) 
Boost_training <- AZ_Data_Boost[Boost_train,]
Boost_testing <-AZ_Data_Boost[ -Boost_train,]  

#Create matrix
Boost_train_labels <- as.numeric(as.factor(Boost_training$PointType)) - 1
Boost_train_matrix <- xgb.DMatrix(data = as.matrix(Boost_training[, -which(names(Boost_training) == "PointType")]), label = Boost_train_labels)

Boost_test_labels <- as.numeric(as.factor(Boost_testing$PointType)) - 1
Boost_test_matrix <- xgb.DMatrix(data = as.matrix(Boost_testing[, -which(names(Boost_testing) == "PointType")]), label = Boost_test_labels)


#parameters
nc <- length(unique(Boost_train_labels))#get two binary response outputs
xgb_params <- list("objective" = "multi:softprob", "eval_metric" = "mlogloss", "num_class" = nc) 

watchlist <- list(Boost_train = Boost_train_matrix, Boost_test = Boost_test_matrix) 
bst_model <-xgb.train(params = xgb_params,
                      data = Boost_train_matrix,
                      nrounds = 500, 
                      watchlist = watchlist,
                      eta = 0.005,
                      max.depth = 7, 
                      subsample = 0.85,
                      verbose = 0)# Use 'eta' learning rate function (which should have values from 0-1) with lower values reducing overfitting. Depth is tree level, 7 resulted in lowest Boost_testing error. Gamma default is 0 (values are 0 to inf): larger values are more conservative and help avoid overfitting. Subsample (values 0-1): lower values also aim to reduce overfitting. The idea is that it controls the number of observations supplied to a tree. We can also determine which variables are allowed to interact, see https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_consBoost_traint.html).

bst_model#We can use the specs from the model to create a graph of the Boost_training and Boost_testing errors
#Boost_training error with Boost_test error as lines
Boost_Outputs_Dataframe <- data.frame(bst_model$evaluation_log)
plot(Boost_Outputs_Dataframe$iter, Boost_Outputs_Dataframe$Boost_train_mlogloss, col='blue', ylim = c(0.0, 1)) 
lines(Boost_Outputs_Dataframe$iter, Boost_Outputs_Dataframe$Boost_test_mlogloss, col='red')


min_training_accuracy <- paste0("Minimum log-loss/training accuracy: ", min(Boost_Outputs_Dataframe$Boost_test_mlogloss, sep=''))
min_training_accuracy

#Feature Importance 
importance <- xgb.importance(colnames(Boost_train_matrix), model = bst_model)
 

#Plot based on Gain: 
# Sort the data by Gain (descending) for better visualization
importance <- importance %>%
  arrange(desc(Gain))

# Plot using ggplot2, ChatGPT was used to help build this plot.
# Create the bar plot with plasma gradient
ggplot(importance, aes(x = reorder(Feature, Gain), y = Gain, fill = Gain)) +
  geom_bar(stat = "identity", color = "black", show.legend = TRUE) +
  scale_fill_viridis_c(option = "plasma") +  # Use the plasma color gradient
  coord_flip() +  # Flip the coordinates for a horizontal bar plot
  labs(
    title = "Feature Importance by Gain",
    x = "Features",
    y = "Gain"
  ) +
  theme_minimal(base_size = 15) +  # Apply a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5, size = 15), # Center and bold title
    axis.title.x = element_text(),
    axis.title.y = element_text(),
    axis.text = element_text(size = 12))

# Make predictions on the Boost_test set
pred_probs <- predict(bst_model, Boost_test_matrix)
pred_labels <- max.col(matrix(pred_probs, ncol = length(unique(Boost_train_labels)))) - 1

# Assess performance
Boost_conf_matrix <- confusionMatrix(factor(pred_labels), factor(Boost_test_labels))

print("Confusion Matrix and Statistics for the XGBoost predictive approach")

print(Boost_conf_matrix)

```



I then implemented a slightly more complicated technique - a neural network - from package 'nnet'.



```{r, message=FALSE, warning = FALSE, echo=FALSE}
set.seed(123)

# Partition the data (80% NN_training, 20% NN_testing)
NN_train_index <- createDataPartition(AZ_Data_Boost$PointType, p = 0.8, list = FALSE)
NN_train_data <- AZ_Data_Boost[NN_train_index, ]
NN_test_data <- AZ_Data_Boost[-NN_train_index, ]


# Normalize the continuous variables
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

NN_train_data[, -which(names(NN_train_data) == "PointType")] <- as.data.frame(lapply(
  NN_train_data[, -which(names(NN_train_data) == "PointType")], normalize
))
NN_test_data[, -which(names(NN_test_data) == "PointType")] <- as.data.frame(lapply(
  NN_test_data[, -which(names(NN_test_data) == "PointType")], normalize
))

# Convert PointType to binary numeric (0, 1)
NN_train_data$PointType <- ifelse(NN_train_data$PointType == "BodyLocation", 1, 0)
NN_test_data$PointType <- ifelse(NN_test_data$PointType == "BodyLocation", 1, 0)

# NN_train the Neural Network
nn_model <- nnet(
  PointType ~ ., 
  data = NN_train_data, 
  size = 10,  # Number of hidden nodes
  decay = 0.01,  # Regularization
  maxit = 200  
)

# Make predictions
NN_test_preds <- predict(nn_model, NN_test_data, type = "raw")
pred_labels <- ifelse(NN_test_preds > 0.5, 1, 0)

# Evaluate the model
NN_conf_matrix <- confusionMatrix(factor(pred_labels), factor(NN_test_data$PointType))
print(NN_conf_matrix)
```

## Results
The relationships between the environmental variables as discerned by the correlation plot were as follows: all forms of radiation were positively correlated, slope and terrain roughness were positively correlated, aspect was negatively correlated with the relative contribution of incoming solar radiation at 9:00AM and positively correlated with the relative contribution of incoming solar radiation at 6:00pm. The relative contribution of incoming solar radiation received at 2:00PM and 6:00PM negatively correlates to that at 9:00AM. 
The best-fit GLM identified the relationships between the variables as follows. The results show that duration of direct solar radiation exposure has a significant negative linear effect (p<0.01) and a significant positive quadratic effect (p<0.002), indicating a non-linear relationship with the point type. Roughness is also a significant positive predictor (p<0.001), suggesting that rougher terrain is associated with higher likelihoods of the point being a mortality site. Overall, the model identifies the duration of direct solar radiation exposure  and terrain roughness as impactful environmental factors influencing the response variable, with a notable quadratic pattern for duration of solar radiation received by that location.The predictive accuracy of the GLM was 59% overall. 
The training and testing XGBoost logloss values (<0.61) were better than random (which would be closer to 0.70). Elevation was the most important environmental feature as measured by gain, followed by the average aridity and MSAVI2 vegetation index. The predictive accuracy of the XGBOOST model was ~50%.
The neural network ceased after 200 iterations and resulted in a 66% accuracy, the highest of the attempted methods.

## Conclusion
Here, I explored various methods used to explore how environmental variables predict UDI mortality locations, with the ultimate goal of identifying key predictors to improve understanding of risk areas. By integrating terrain generated from a high-resolution digital elevation model and subsequent derivatives — including solar radiation, vegetation density, and aridity - I evaluated patterns in the data and tested their predictive power using GLMs, AIC scores, XGBoost, and neural nets. Results showed that elevation, aridity, and vegetation were among the most influential factors, with predictive accuracies ranging from 50% to 66% depending on the method used. Ultimately, the neural net was the most accurate, although: with additional variable examinations, data preprocessing, and tuning, it is possible that these accuracy values could increase. These findings highlight the value of incorporating detailed environmental metrics into predictive models to better understand the spatial risks associated with UDI mortality, offering potential applications for more targeted mitigation efforts in the Tucson Sector.

## References
Martinez, Daniel, Robin Reineke, Bruce Anderson, Gregory Hess, and Bruce Parks. “Migrant Deaths in Southern Arizona: Recovered Undocumented Border Crosser Remains Investigated by the Pima County Office of the Medical Examiner, 1990-2020.” J Hum Secur 1(2021): 257–286. 
US Customs and Border Protection. 2022a. “Nationwide Enforcement Encounters: Title 8 Enforcement Actions and Title 42 Expulsions.” https://www.cbp.gov/newsroom/stats/cbp-enforcement-statistics/title-8-and-title-42-statistics-fy2020. 
US Customs and Border Protection. 2022b. “Southwest Land Border Encounters (By Component). Fiscal Year 2022.” https://www.cbp.gov/newsroom/stats/southwest-land-border-encounters-by-component. 
Boyce, G. A., S. N. Chambers, and S. Launius. 2019. “Bodily Inertia and the Weaponization of the Sonoran Desert in US Boundary Enforcement: A GIS Modeling of Migration Routes through Arizona’s Altar Valley.” Journal on Migration and Human Security 7(1): 23–35.   
Giordano, Alberto, and Katherine Spradley. 2017. “Migrant Deaths at the Arizona–Mexico Border: Spatial Trends of a Mass Disaster.” Forensic Science International 280(1):200–212.   
Foster, Kevin M., Brendon A. Bradley, Christopher R. McGann, and Liam M. Wotherspoon. 2019. “A VS30 Map for New Zealand Based on Geologic and Terrain Proxy Variables and Field Measurements.” Earthquake Spectra 35(4):1865–1897.   